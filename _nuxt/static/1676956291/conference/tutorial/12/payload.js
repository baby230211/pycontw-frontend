__NUXT_JSONP__("/conference/tutorial/12", (function(a){return {data:[{speechData:{id:12,begin_time:"2022-09-03T05:05:00Z",end_time:"2022-09-03T06:35:00Z",is_remote:false,location:"1-r3",registration_link:a,youtube_id:a,title:"Effective ways to scale-up and maintain your Web Crawling project using Scrapy and its ecosystem of tools.",category:"TOOL",language:"ENEN",python_level:"INTERMEDIATE",recording_policy:true,abstract:"Acquiring massive amounts of public data from anywhere on the web is crucial in today's data age.  Such undertaking could be achieved through the use of Spiders which has two components: (1) Crawling —— the means to find the content of interest and (2) Extraction —— the means of turning data into a structured format. However, the web changes so fast that scaling and maintaining these spiders become an issue. In this talk, we will create an end-to-end web crawling project that walks through each crucial step, the challenges for each stage, and the available tools and techniques to overcome such obstacles. We will be using Scrapy, one of the most popular web crawling Python frameworks, together with its ecosystem of tools.",detailed_description:"**Title**\r\nEffective ways to maintain and scale-up your Web Crawling project using Scrapy and it's ecosystem of tools.\r\n**Category**\r\nProject Tooling\r\n**Duration**\r\n1.5 hrs\r\n**Language**\r\nEnglish\r\n\r\nAcquiring massive amounts of public data from anywhere on the web is crucial in today's data age.  Such undertaking could be achieved through the use of Spiders which has two components: (1) Crawling —— the means to find the content of interest and (2) Extraction —— the means of turning data into a structured format. However, the web changes so fast that maintaining and scaling these spiders become an issue. In this talk, we will create an end-to-end web crawling project that walks through each crucial step, the challenges for each stage, and the available tools and techniques to overcome such obstacles. We will be using Scrapy, one of the most popular web crawling Python frameworks, together with its ecosystem of tools.\r\n\r\n- [Presentation Slides](https:\u002F\u002Fdocs.google.com\u002Fpresentation\u002Fd\u002F1ow378UmfvoxzNaFjFWYq4u51b5pTKu0GAORar3umunY\u002Fedit?usp=sharing)\r\n- [Code shown in the talk](https:\u002F\u002Fgithub.com\u002FBurnzZ\u002Fmy-conference-presentations\u002Ftree\u002Fmaster\u002Fpycon-apac-2022\u002Fbooks_to_scrape)\r\n\r\n### Core libraries used:\r\n\r\n  - [scrapy] One of the most popular libraries for web crawling in Python\r\n  - [spidermon] Scrapy extension for monitoring spider execution\r\n  - [web-poet] Writing Page Object patterns for web data extraction\r\n  - [scrapy-poet] Scrapy integration for web-poet\r\n  - [jsonschema] allows you to annotate and validate JSON documents\r\n  - [scrapy-jsonschema] Scrapy integration for jsonschema\r\n  - [playwright] handle browsers like Chromium and Firefox.\r\n  - [scrapy-playwright] Scrapy integration for playwright\r\n\r\n[scrapy]: https:\u002F\u002Fscrapy.org\u002F\r\n[spidermon]: https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fspidermon\r\n[web-poet]: https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fweb-poet\r\n[scrapy-poet]: https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fscrapy-poet\r\n[jsonschema]: https:\u002F\u002Fpypi.org\u002Fproject\u002Fjsonschema\u002F\r\n[scrapy-jsonschema]: https:\u002F\u002Fgithub.com\u002Fscrapy-plugins\u002Fscrapy-jsonschema\r\n[playwright]: https:\u002F\u002Fgithub.com\u002Fmicrosoft\u002Fplaywright-python\r\n[scrapy-playwright]: https:\u002F\u002Fgithub.com\u002Fscrapy-plugins\u002Fscrapy-playwright\r\n\r\n### Helper libraries:\r\n\r\n- [SpiderKeeper](https:\u002F\u002Fgithub.com\u002FDormyMo\u002FSpiderKeeper)\r\n- [number-parser](https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fnumber-parser)\r\n- [dateparser](https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fdateparser)\r\n- [maya](https:\u002F\u002Fgithub.com\u002Ftimofurrer\u002Fmaya)\r\n- [priceparser](https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fprice-parser)\r\n- [extruct](https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fextruct)\r\n- [html-text](https:\u002F\u002Fgithub.com\u002FTeamHG-Memex\u002Fhtml-text)\r\n- [scrapy-deltafetch](https:\u002F\u002Fgithub.com\u002Fscrapy-plugins\u002Fscrapy-deltafetch)\r\n- [scrapyrt](https:\u002F\u002Fgithub.com\u002Fscrapinghub\u002Fscrapyrt)\r\n- [autopager](https:\u002F\u002Fgithub.com\u002FTeamHG-Memex\u002Fautopager)\r\n\r\n### Enterprise Tools:\r\n\r\n- [AutoExtract](https:\u002F\u002Fwww.zyte.com\u002Fautomated-data-extraction)\r\n- [Zyte API](https:\u002F\u002Fdocs.zyte.com\u002Fzyte-api\u002Fget-started.html)\r\n- [Scrapy Cloud](https:\u002F\u002Fwww.zyte.com\u002Fscrapy-cloud)",slide_link:a,slido_embed_link:"https:\u002F\u002Fapp.sli.do\u002Fevent\u002F6DJ5vhaAUaAP3as8m7vcnw",hackmd_embed_link:"https:\u002F\u002Fhackmd.io\u002F@pycontw\u002Fryg0L67ys",speakers:[{thumbnail_url:"https:\u002F\u002Ftw.pycon.org\u002Ftemp\u002Fmedia\u002Fcache\u002F2f\u002F4a\u002F2f4aeba65d14c71ddcf229469d85a169.jpg",name:"Kevin Lloyd Bernal",github_profile_url:"https:\u002F\u002Fgithub.com\u002FBurnzZ",twitter_profile_url:"https:\u002F\u002Ftwitter.com\u002FKevinCookinData",facebook_profile_url:a,bio:"Kevin is currently a Software Engineer in Zyte. He builds on solutions to crawl the web at scale. He's part of the team that develops and maintains open source packages that enable developers to effectively manage their parsing and crawling solutions. He is also currently studying MS in Computer Science at GA Tech specializing in Machine Learning."}],event_type:"tutorial"}}],fetch:{},mutations:[]}}("")));